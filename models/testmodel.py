# í† í¬ë‚˜ì´ì € ë° ëª¨ë¸ ë¡œë“œ
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

# KoBERT í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained("monologg/kobert", trust_remote_code=True)
model = AutoModelForSequenceClassification.from_pretrained("rkdaldus/ko-sent5-classification")

# ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
text = "ì˜¤ëŠ˜ ë¯¸ë‹ˆ í”„ë¡œì íŠ¸ ë°œí‘œë‚ ì´ì•¼ ì¼ì£¼ì¼ë™ì•ˆ ì ë„ ëª»ìê³  ë“œë””ì–´ ëë‚œë‹¤ íœ´~"
inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True)
with torch.no_grad():
    outputs = model(**inputs)
predicted_label = torch.argmax(outputs.logits, dim=1).item()

# ê°ì • ë ˆì´ë¸” ì •ì˜
emotion_labels = {
    0: ("Angry", "ğŸ˜¡"),
    1: ("Fear", "ğŸ˜¨"),
    2: ("Happy", "ğŸ˜Š"),
    3: ("Tender", "ğŸ¥°"),
    4: ("Sad", "ğŸ˜¢")
}

# ì˜ˆì¸¡ëœ ê°ì • ì¶œë ¥
print(f"ì˜ˆì¸¡ëœ ê°ì •: {emotion_labels[predicted_label][0]} {emotion_labels[predicted_label][1]}")
